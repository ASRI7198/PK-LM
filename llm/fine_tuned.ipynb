{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lolol\\anaconda3\\envs\\llm\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99b14ce22ff4dcb95e6cea9c674a29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.793, 'grad_norm': 16.11667251586914, 'learning_rate': 4.9423963133640554e-05, 'epoch': 0.12}\n",
      "{'loss': 1.645, 'grad_norm': 18.171791076660156, 'learning_rate': 4.8847926267281106e-05, 'epoch': 0.23}\n",
      "{'loss': 1.4873, 'grad_norm': 13.069515228271484, 'learning_rate': 4.827188940092166e-05, 'epoch': 0.35}\n",
      "{'loss': 1.4074, 'grad_norm': 13.665339469909668, 'learning_rate': 4.7695852534562216e-05, 'epoch': 0.46}\n",
      "{'loss': 1.2604, 'grad_norm': 11.04793643951416, 'learning_rate': 4.711981566820277e-05, 'epoch': 0.58}\n",
      "{'loss': 1.2453, 'grad_norm': 14.253957748413086, 'learning_rate': 4.654377880184332e-05, 'epoch': 0.69}\n",
      "{'loss': 1.234, 'grad_norm': 11.726035118103027, 'learning_rate': 4.596774193548387e-05, 'epoch': 0.81}\n",
      "{'loss': 1.1952, 'grad_norm': 11.10894775390625, 'learning_rate': 4.539170506912442e-05, 'epoch': 0.92}\n",
      "{'loss': 1.2199, 'grad_norm': 13.73951244354248, 'learning_rate': 4.4815668202764974e-05, 'epoch': 1.04}\n",
      "{'loss': 1.109, 'grad_norm': 11.516401290893555, 'learning_rate': 4.423963133640553e-05, 'epoch': 1.15}\n",
      "{'loss': 1.0506, 'grad_norm': 12.129411697387695, 'learning_rate': 4.366359447004609e-05, 'epoch': 1.27}\n",
      "{'loss': 1.076, 'grad_norm': 10.55134391784668, 'learning_rate': 4.308755760368664e-05, 'epoch': 1.38}\n",
      "{'loss': 1.0712, 'grad_norm': 10.74219036102295, 'learning_rate': 4.2511520737327194e-05, 'epoch': 1.5}\n",
      "{'loss': 1.0232, 'grad_norm': 13.410590171813965, 'learning_rate': 4.1935483870967746e-05, 'epoch': 1.61}\n",
      "{'loss': 1.0537, 'grad_norm': 11.49085521697998, 'learning_rate': 4.13594470046083e-05, 'epoch': 1.73}\n",
      "{'loss': 1.0451, 'grad_norm': 13.962566375732422, 'learning_rate': 4.078341013824885e-05, 'epoch': 1.84}\n",
      "{'loss': 0.9899, 'grad_norm': 9.305298805236816, 'learning_rate': 4.02073732718894e-05, 'epoch': 1.96}\n",
      "{'loss': 0.9765, 'grad_norm': 9.846336364746094, 'learning_rate': 3.963133640552996e-05, 'epoch': 2.07}\n",
      "{'loss': 0.9554, 'grad_norm': 13.539144515991211, 'learning_rate': 3.905529953917051e-05, 'epoch': 2.19}\n",
      "{'loss': 0.9582, 'grad_norm': 9.786081314086914, 'learning_rate': 3.847926267281106e-05, 'epoch': 2.3}\n",
      "{'loss': 0.9607, 'grad_norm': 9.681340217590332, 'learning_rate': 3.7903225806451614e-05, 'epoch': 2.42}\n",
      "{'loss': 0.97, 'grad_norm': 10.61030387878418, 'learning_rate': 3.7327188940092166e-05, 'epoch': 2.53}\n",
      "{'loss': 0.9578, 'grad_norm': 10.330808639526367, 'learning_rate': 3.675115207373272e-05, 'epoch': 2.65}\n",
      "{'loss': 0.9515, 'grad_norm': 9.181556701660156, 'learning_rate': 3.6175115207373276e-05, 'epoch': 2.76}\n",
      "{'loss': 0.9596, 'grad_norm': 8.990553855895996, 'learning_rate': 3.559907834101383e-05, 'epoch': 2.88}\n",
      "{'loss': 0.9562, 'grad_norm': 10.798639297485352, 'learning_rate': 3.502304147465438e-05, 'epoch': 3.0}\n",
      "{'loss': 0.9107, 'grad_norm': 9.770209312438965, 'learning_rate': 3.444700460829493e-05, 'epoch': 3.11}\n",
      "{'loss': 0.9059, 'grad_norm': 10.037028312683105, 'learning_rate': 3.387096774193548e-05, 'epoch': 3.23}\n",
      "{'loss': 0.8926, 'grad_norm': 8.78221607208252, 'learning_rate': 3.3294930875576034e-05, 'epoch': 3.34}\n",
      "{'loss': 0.904, 'grad_norm': 7.539056777954102, 'learning_rate': 3.271889400921659e-05, 'epoch': 3.46}\n",
      "{'loss': 0.9242, 'grad_norm': 10.20564079284668, 'learning_rate': 3.2142857142857144e-05, 'epoch': 3.57}\n",
      "{'loss': 0.9019, 'grad_norm': 9.688573837280273, 'learning_rate': 3.15668202764977e-05, 'epoch': 3.69}\n",
      "{'loss': 0.8903, 'grad_norm': 8.87767219543457, 'learning_rate': 3.0990783410138254e-05, 'epoch': 3.8}\n",
      "{'loss': 0.8999, 'grad_norm': 11.032483100891113, 'learning_rate': 3.0414746543778806e-05, 'epoch': 3.92}\n",
      "{'loss': 0.8744, 'grad_norm': 9.228084564208984, 'learning_rate': 2.9838709677419357e-05, 'epoch': 4.03}\n",
      "{'loss': 0.837, 'grad_norm': 8.317304611206055, 'learning_rate': 2.926267281105991e-05, 'epoch': 4.15}\n",
      "{'loss': 0.8509, 'grad_norm': 9.615741729736328, 'learning_rate': 2.8686635944700464e-05, 'epoch': 4.26}\n",
      "{'loss': 0.8748, 'grad_norm': 9.842369079589844, 'learning_rate': 2.8110599078341016e-05, 'epoch': 4.38}\n",
      "{'loss': 0.8354, 'grad_norm': 10.89796257019043, 'learning_rate': 2.7534562211981567e-05, 'epoch': 4.49}\n",
      "{'loss': 0.85, 'grad_norm': 9.692361831665039, 'learning_rate': 2.6958525345622122e-05, 'epoch': 4.61}\n",
      "{'loss': 0.8469, 'grad_norm': 7.4955668449401855, 'learning_rate': 2.6382488479262674e-05, 'epoch': 4.72}\n",
      "{'loss': 0.8754, 'grad_norm': 9.34558391571045, 'learning_rate': 2.5806451612903226e-05, 'epoch': 4.84}\n",
      "{'loss': 0.8503, 'grad_norm': 9.053627014160156, 'learning_rate': 2.523041474654378e-05, 'epoch': 4.95}\n",
      "{'loss': 0.8458, 'grad_norm': 8.152688026428223, 'learning_rate': 2.4654377880184332e-05, 'epoch': 5.07}\n",
      "{'loss': 0.8051, 'grad_norm': 6.015848159790039, 'learning_rate': 2.4078341013824887e-05, 'epoch': 5.18}\n",
      "{'loss': 0.7799, 'grad_norm': 7.634792804718018, 'learning_rate': 2.350230414746544e-05, 'epoch': 5.3}\n",
      "{'loss': 0.8036, 'grad_norm': 8.638733863830566, 'learning_rate': 2.2926267281105994e-05, 'epoch': 5.41}\n",
      "{'loss': 0.8099, 'grad_norm': 9.493497848510742, 'learning_rate': 2.2350230414746546e-05, 'epoch': 5.53}\n",
      "{'loss': 0.7967, 'grad_norm': 12.549450874328613, 'learning_rate': 2.1774193548387097e-05, 'epoch': 5.65}\n",
      "{'loss': 0.8321, 'grad_norm': 9.935140609741211, 'learning_rate': 2.1198156682027652e-05, 'epoch': 5.76}\n",
      "{'loss': 0.81, 'grad_norm': 10.760689735412598, 'learning_rate': 2.0622119815668204e-05, 'epoch': 5.88}\n",
      "{'loss': 0.7915, 'grad_norm': 9.224417686462402, 'learning_rate': 2.0046082949308755e-05, 'epoch': 5.99}\n",
      "{'loss': 0.7718, 'grad_norm': 7.498614311218262, 'learning_rate': 1.9470046082949307e-05, 'epoch': 6.11}\n",
      "{'loss': 0.7732, 'grad_norm': 7.6960225105285645, 'learning_rate': 1.8894009216589862e-05, 'epoch': 6.22}\n",
      "{'loss': 0.7625, 'grad_norm': 6.770432949066162, 'learning_rate': 1.8317972350230417e-05, 'epoch': 6.34}\n",
      "{'loss': 0.7477, 'grad_norm': 7.460851669311523, 'learning_rate': 1.774193548387097e-05, 'epoch': 6.45}\n",
      "{'loss': 0.7536, 'grad_norm': 8.48767375946045, 'learning_rate': 1.7165898617511524e-05, 'epoch': 6.57}\n",
      "{'loss': 0.7815, 'grad_norm': 10.916521072387695, 'learning_rate': 1.6589861751152075e-05, 'epoch': 6.68}\n",
      "{'loss': 0.774, 'grad_norm': 5.4969377517700195, 'learning_rate': 1.6013824884792627e-05, 'epoch': 6.8}\n",
      "{'loss': 0.7673, 'grad_norm': 6.967611312866211, 'learning_rate': 1.543778801843318e-05, 'epoch': 6.91}\n",
      "{'loss': 0.7543, 'grad_norm': 12.326732635498047, 'learning_rate': 1.4861751152073732e-05, 'epoch': 7.03}\n",
      "{'loss': 0.7253, 'grad_norm': 9.094114303588867, 'learning_rate': 1.4285714285714285e-05, 'epoch': 7.14}\n",
      "{'loss': 0.7393, 'grad_norm': 8.935373306274414, 'learning_rate': 1.3709677419354839e-05, 'epoch': 7.26}\n",
      "{'loss': 0.7408, 'grad_norm': 9.4608793258667, 'learning_rate': 1.313364055299539e-05, 'epoch': 7.37}\n",
      "{'loss': 0.7321, 'grad_norm': 11.092326164245605, 'learning_rate': 1.2557603686635947e-05, 'epoch': 7.49}\n",
      "{'loss': 0.7445, 'grad_norm': 8.892862319946289, 'learning_rate': 1.1981566820276497e-05, 'epoch': 7.6}\n",
      "{'loss': 0.7209, 'grad_norm': 9.073614120483398, 'learning_rate': 1.1405529953917052e-05, 'epoch': 7.72}\n",
      "{'loss': 0.6924, 'grad_norm': 8.148392677307129, 'learning_rate': 1.0829493087557604e-05, 'epoch': 7.83}\n",
      "{'loss': 0.7045, 'grad_norm': 11.797697067260742, 'learning_rate': 1.0253456221198157e-05, 'epoch': 7.95}\n",
      "{'loss': 0.7206, 'grad_norm': 10.917017936706543, 'learning_rate': 9.67741935483871e-06, 'epoch': 8.06}\n",
      "{'loss': 0.6851, 'grad_norm': 13.209821701049805, 'learning_rate': 9.101382488479262e-06, 'epoch': 8.18}\n",
      "{'loss': 0.6959, 'grad_norm': 11.2012939453125, 'learning_rate': 8.525345622119817e-06, 'epoch': 8.29}\n",
      "{'loss': 0.6583, 'grad_norm': 9.33452033996582, 'learning_rate': 7.949308755760369e-06, 'epoch': 8.41}\n",
      "{'loss': 0.6783, 'grad_norm': 6.961407661437988, 'learning_rate': 7.373271889400922e-06, 'epoch': 8.53}\n",
      "{'loss': 0.6756, 'grad_norm': 8.94405460357666, 'learning_rate': 6.7972350230414745e-06, 'epoch': 8.64}\n",
      "{'loss': 0.69, 'grad_norm': 8.118705749511719, 'learning_rate': 6.221198156682028e-06, 'epoch': 8.76}\n",
      "{'loss': 0.7149, 'grad_norm': 6.9185566902160645, 'learning_rate': 5.64516129032258e-06, 'epoch': 8.87}\n",
      "{'loss': 0.6836, 'grad_norm': 8.8040771484375, 'learning_rate': 5.0691244239631346e-06, 'epoch': 8.99}\n",
      "{'loss': 0.6629, 'grad_norm': 7.864955902099609, 'learning_rate': 4.493087557603687e-06, 'epoch': 9.1}\n",
      "{'loss': 0.6669, 'grad_norm': 7.654131889343262, 'learning_rate': 3.9170506912442395e-06, 'epoch': 9.22}\n",
      "{'loss': 0.6495, 'grad_norm': 7.9505720138549805, 'learning_rate': 3.341013824884793e-06, 'epoch': 9.33}\n",
      "{'loss': 0.6837, 'grad_norm': 10.18721866607666, 'learning_rate': 2.7649769585253458e-06, 'epoch': 9.45}\n",
      "{'loss': 0.6382, 'grad_norm': 14.059433937072754, 'learning_rate': 2.1889400921658987e-06, 'epoch': 9.56}\n",
      "{'loss': 0.67, 'grad_norm': 7.024914741516113, 'learning_rate': 1.6129032258064516e-06, 'epoch': 9.68}\n",
      "{'loss': 0.6565, 'grad_norm': 9.287586212158203, 'learning_rate': 1.0368663594470047e-06, 'epoch': 9.79}\n",
      "{'loss': 0.673, 'grad_norm': 7.6845703125, 'learning_rate': 4.6082949308755763e-07, 'epoch': 9.91}\n",
      "{'train_runtime': 3705.5953, 'train_samples_per_second': 2.34, 'train_steps_per_second': 1.171, 'train_loss': 0.8925883715053857, 'epoch': 10.0}\n",
      "âœ… ModÃ¨le fine-tunÃ© sauvegardÃ© dans fine_tuned_gpt2-3\n"
     ]
    }
   ],
   "source": [
    "    from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "    from torch.utils.data import Dataset\n",
    "    import torch\n",
    "\n",
    "    # Nom du modÃ¨le et des fichiers\n",
    "    MODEL_NAME = \"gpt2\"\n",
    "    DATA_PATH = \"data_team.txt\"  # Fichier avec les questions et Ã©quipes\n",
    "    OUTPUT_DIR = \"fine_tuned_gpt2-3\"\n",
    "\n",
    "    # Charger le tokenizer et le modÃ¨le prÃ©-entraÃ®nÃ©\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # Ajouter des tokens spÃ©ciaux\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})  # Token de padding\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # DÃ©finition du Dataset\n",
    "    class PokemonTeamDataset(Dataset):\n",
    "        def __init__(self, file_path, tokenizer, block_size):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = file.read()\n",
    "\n",
    "            # SÃ©parer les exemples avec \"---\"\n",
    "            examples = data.split(\"---\")\n",
    "\n",
    "            # Tokeniser chaque exemple (ajoute implicitement \"Donne-moi une Ã©quipe :\" au dÃ©but)\n",
    "            self.examples = [\n",
    "                tokenizer(f\"{example.strip()}\", max_length=block_size, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "                for example in examples if example.strip()\n",
    "            ]\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.examples)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return torch.tensor(self.examples[idx])\n",
    "\n",
    "    # ParamÃ¨tres\n",
    "    BLOCK_SIZE = 128  # Taille des tokens\n",
    "    train_dataset = PokemonTeamDataset(DATA_PATH, tokenizer, BLOCK_SIZE)\n",
    "\n",
    "    # Collator pour gÃ©rer le padding\n",
    "    from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # On entraÃ®ne en mode gÃ©nÃ©ration, pas en masquage\n",
    "    )\n",
    "\n",
    "    # Configuration de l'entraÃ®nement\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=10,  # Augmenter le nombre d'Ã©poques pour un meilleur apprentissage\n",
    "        per_device_train_batch_size=2,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "        logging_steps=50,\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"no\",\n",
    "    )\n",
    "\n",
    "    # Instancier Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    # ðŸš€ EntraÃ®nement\n",
    "    trainer.train()\n",
    "\n",
    "    # Sauvegarde du modÃ¨le et du tokenizer\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    print(f\"âœ… ModÃ¨le fine-tunÃ© sauvegardÃ© dans {OUTPUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
